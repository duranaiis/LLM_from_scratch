{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e19929af",
   "metadata": {},
   "source": [
    "# Тарау 1: Мәтіндерді өңдеу\n",
    "# Раздел 1: Обработка текстовых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17946c7",
   "metadata": {},
   "source": [
    "1. Бұл блокнотта қолданылатын пакеттер:\n",
    "2. Библиотеки, которые используются в этом блокноте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d86cde18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.9.0\n",
      "tiktoken version: 0.12.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0a0f4a",
   "metadata": {},
   "source": [
    "- Бұл тарауда  деректерді өңдеу және бөлшектерге бөлу қарастырылады\n",
    "- В этой главе мы рассмотрим обработку текстовых данных и разделение их на части."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0889af3b",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/01.webp?timestamp=1\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416bc000",
   "metadata": {},
   "source": [
    "## 2.1 Cөз эмбедингтерін (векторлық көріністер) түсіндіру\n",
    "## 2.1 Понимание эмбеддингов (векторные представления) слов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1c92b1",
   "metadata": {},
   "source": [
    "Эмбеддингтердің көптеген түрлері бар; бұл жұмыста біз мәтіндік эмбеддингтерге назар аударамыз.\n",
    "\n",
    "Существует множество видов эмбеддингов; в этой работе мы сосредоточимся на текстовых эмбеддингах.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608d9190",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/02.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ef2894",
   "metadata": {},
   "source": [
    "- Ауқымды Тіл Модельдері (ATM) жоғары өлшемді кеңістіктердегі (яғни, мыңдаған өлшемдері бар) эмбеддингтермен жұмыс істейді.\n",
    "- Языковые модели работают с эмбеддингами в высокоразмерных пространствах (т. е. с тысячами измерений).\n",
    "***\n",
    "- Біз мұндай жоғары өлшемді кеңістіктерді көрнекі түрде елестете алмайтындықтан (өйткені адамдар 1, 2 немесе 3 өлшеммен ойлайды),     \n",
    "төмендегі суретте 2 өлшемді эмбеддинг кеңістігі көрсетілген.\n",
    "- Поскольку мы не можем визуализировать такие многомерные пространства (ведь люди мыслят в 1, 2 или 3 измерениях),    \n",
    "на рисунке ниже показано двумерное пространство эмбеддингов.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dd6279",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/03.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eff7c1",
   "metadata": {},
   "source": [
    "## 2.2 Текст токендеу\n",
    "## 2.2 Токенизация текста"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4bda65",
   "metadata": {},
   "source": [
    "- Бұл бөлімде біз мәтінді токендейміз, яғни оны жеке сөздер мен тыныс белгілері сияқты кішірек бірліктерге бөлеміз.\n",
    "- В этом разделе мы токенизируем текст, то есть разделяем его на более мелкие единицы, такие как отдельные слова и знаки препинания."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfb4241",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/04.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10e9049",
   "metadata": {},
   "source": [
    "- Бастапқы мәтін ретінде Герберт Уэлс жазушының \"Time Machine\" шығармасын қолданамыз.\n",
    "- В качестве исходного текста мы используем произведение Герберта Уэлса \"Time Machine\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3046bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "if not os.path.exists(\"time_machine.txt\"):\n",
    "    url = (\"https://raw.githubusercontent.com/Azamat0315277/LLM_from_scratch/refs/heads/main/ch01/time_machine.txt\")\n",
    "    file_path = \"time_machine.txt\"\n",
    "    urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d1a3594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 197730\n",
      "The Time Traveller (for so it will be convenient to speak of him) was\n",
      "expounding a recondite matter to us. His pale grey eyes shone and\n",
      "twinkled, and his usually pale face was flus\n"
     ]
    }
   ],
   "source": [
    "with open(\"time_machine.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[20:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f2704b",
   "metadata": {},
   "source": [
    "- Мақсат — осы мәтінді АТМ үшін токендеу және одан эмбединг жасау.  \n",
    "- Кейінірек жоғарыдағы мәтінге қолдану үшін, қарапайым үлгі мәтін негізінде қарапайым токендегіш әзірлейміз.   \n",
    "- Төмендегі тұрақты өрнек (Regular Expression)  сөздерді бос орындар бойынша бөледі.    \n",
    "\n",
    " <br>\n",
    " \n",
    "- Цель — токенизировать текст и создать эмбеддинги для LLM.  \n",
    "- Давайте разработаем простой токенизатор на основе простого примера текста, который мы затем сможем применить к тексту выше.\n",
    "- Следующее регулярное выражение (Regular Expression) будет выполнять разделение по пробельным символам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58c502a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62d7fa6",
   "metadata": {},
   "source": [
    "- Бізге мәтінді тек бос орындар бойынша ғана емес, үтірлер мен нүктелер бойынша да бөлу керек, сондықтан осыған сәйкес тұрақты өрнекті өзгертейік.\n",
    "- Нам необходимо выполнять разделение не только по пробельным символам, но и по запятым и точкам, поэтому давайте изменим регулярное выражение чтобы оно учитывало и это."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5176f9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310f344c",
   "metadata": {},
   "source": [
    "- Көріп тұрғанымыздай, бұл сөз арасындағы бос орындарды көбейтті, сондықтан оларды да алып тастайық.\n",
    "- Как мы видим, это создаёт пустые строки — давайте и их удалим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db3841fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "# Strip whitespace from each item and then filter out any empty strings.\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5872483c",
   "metadata": {},
   "source": [
    "- Бұл әжептәуір жақсы, бірақ нүктелер, сұрақ белгілері және т.б. сияқты тыныс белгілерінің басқа түрлерін де ескерейік.\n",
    "- Выглядит неплохо, но давайте также обработаем и другие знаки препинания, такие как точки, вопросительные знаки и так далее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a53f894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '—', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world—. Is this-- a test?\"\n",
    "\n",
    "result = re.split(r'([,.:;?_!\"()—\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e2d034",
   "metadata": {},
   "source": [
    "- Нәтиже жақсы, енді біз осы токендеу әдісін бастапқы мәтінге қолдануға дайынбыз.\n",
    "- Очень неплохо, и теперь мы готовы применить эту токенизацию к исходному тексту."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198e28ed",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/05.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2160eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', '.', 'Introduction', 'The', 'Time', 'Traveller', '(', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him', ')', 'was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', '.', 'His', 'pale', 'grey', 'eyes']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()—*’“$%\\']|--|——|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db016d52",
   "metadata": {},
   "source": [
    "- Токендердің жалпы санын есептейік.\n",
    "- Давайте посчитаем общее количество токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b121e3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41376\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9250b19",
   "metadata": {},
   "source": [
    "## 2.3 Токендерді токен ID-ларына айналдыру\n",
    "## 2.3 Преобразование токенов в идентификаторы (ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1241c1",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/06.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c02b7ae",
   "metadata": {},
   "source": [
    "- Енді осы токендерден барлық бірегей токендерді қамтитын сөздік құра аламыз.\n",
    "- Теперь из этих токенов можно составить словарь, состоящий из всех уникальных токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0d8d3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5442\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3052e13",
   "metadata": {},
   "source": [
    "- Сөздік құру\n",
    "- Составляем словарь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07dcbc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20437bb1",
   "metadata": {},
   "source": [
    "- Сөздіктегі алғашқы 50 сөзді көрсету\n",
    "- Ниже приведены первые 50 элементов этого словаря:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ceadeeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('$', 1)\n",
      "('%', 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "('*', 5)\n",
      "(',', 6)\n",
      "('-', 7)\n",
      "('.', 8)\n",
      "('000', 9)\n",
      "('1', 10)\n",
      "('1500', 11)\n",
      "('2', 12)\n",
      "('20', 13)\n",
      "('2001', 14)\n",
      "('3', 15)\n",
      "('30', 16)\n",
      "('4', 17)\n",
      "('5', 18)\n",
      "('50', 19)\n",
      "('501', 20)\n",
      "('596-1887', 21)\n",
      "('6', 22)\n",
      "('60', 23)\n",
      "('64-6221541', 24)\n",
      "('7', 25)\n",
      "('8', 26)\n",
      "('801', 27)\n",
      "('809', 28)\n",
      "('84116', 29)\n",
      "('9', 30)\n",
      "('90', 31)\n",
      "(':', 32)\n",
      "(';', 33)\n",
      "('?', 34)\n",
      "('A', 35)\n",
      "('ACTUAL', 36)\n",
      "('AGREE', 37)\n",
      "('AGREEMENT', 38)\n",
      "('AND', 39)\n",
      "('ANY', 40)\n",
      "('ANYTHING', 41)\n",
      "('ASCII”', 42)\n",
      "('About', 43)\n",
      "('Above', 44)\n",
      "('Abruptly', 45)\n",
      "('Accordingly', 46)\n",
      "('Additional', 47)\n",
      "('Advancement', 48)\n",
      "('Africa', 49)\n",
      "('After', 50)\n",
      "('Afterwards', 51)\n",
      "('Again', 52)\n",
      "('Age', 53)\n",
      "('Ages', 54)\n",
      "('Agreed', 55)\n",
      "('All', 56)\n",
      "('Allen', 57)\n",
      "('Already', 58)\n",
      "('Also', 59)\n",
      "('Although', 60)\n",
      "('Amateur', 61)\n",
      "('America', 62)\n",
      "('An', 63)\n",
      "('And', 64)\n",
      "('Any', 65)\n",
      "('Apparently', 66)\n",
      "('Archive', 67)\n",
      "('Are', 68)\n",
      "('Around', 69)\n",
      "('As', 70)\n",
      "('At', 71)\n",
      "('B', 72)\n",
      "('BE', 73)\n",
      "('BEFORE', 74)\n",
      "('BREACH', 75)\n",
      "('BUT', 76)\n",
      "('Badly', 77)\n",
      "('Banstead', 78)\n",
      "('Battersea', 79)\n",
      "('Battle', 80)\n",
      "('Be', 81)\n",
      "('Because', 82)\n",
      "('Before', 83)\n",
      "('Behaviour', 84)\n",
      "('Belemnite', 85)\n",
      "('Below', 86)\n",
      "('Beneath', 87)\n",
      "('Besides', 88)\n",
      "('Better', 89)\n",
      "('Between', 90)\n",
      "('Beyond', 91)\n",
      "('Blank', 92)\n",
      "('Breadth', 93)\n",
      "('Brontosaurus', 94)\n",
      "('Burslem', 95)\n",
      "('But', 96)\n",
      "('By', 97)\n",
      "('C', 98)\n",
      "('CONSEQUENTIAL', 99)\n",
      "('CONTRACT', 100)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 100:\n",
    "       break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df69bfb",
   "metadata": {},
   "source": [
    "## Link:\n",
    "* Tokenizer visualizer: https://tiktokenizer.vercel.app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8c6e56",
   "metadata": {},
   "source": [
    "- Төменде шағын сөздікті пайдаланып, қысқа үлгі мәтінді токендеу үдерісі көрсетілген:\n",
    "- Ниже показана токенизация небольшого образца текста с использованием маленького словаря:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe3fb0c",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/07.webp?123\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdac41d",
   "metadata": {},
   "source": [
    "- Енді барлығын токендеу `Класына` біріктіреміз:\n",
    "- Теперь соберём всё вместе в `Kласс` токенизатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d98f8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        # Preprocess the text\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()—*’“$%\\']|--|——|\\s)', text)\n",
    "        # Remove empty strings\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        # Convert strings to IDs\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?_!\"()—*’“$%\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724e2f5f",
   "metadata": {},
   "source": [
    "- `encode` функциясы **мәтінді** **токен ID-ларына** айналдырады.\n",
    "- `decode` функциясы **токен ID-ларын** **кері мәтінге** айналдырады.\n",
    "\n",
    "<br>\n",
    "\n",
    "- Функция `encode` преобразует **текст** в **идентификаторы токенов (ID)**.\n",
    "- Функция `decode` преобразует **идентификаторы токенов (ID)** обратно в **текст**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b604cc",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/08.webp?123\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2e9e35",
   "metadata": {},
   "source": [
    "- Біз токендеу құралын пайдаланып, **мәтіндерді** **бүтін сандарға** кодтай (яғни, токендей) аламыз.\n",
    "- Содан кейін бұл **бүтін сандарды** АТМ үшін кіріс деректері ретінде **эмбеддинг жасауға** болады."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc827f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[596, 6, 259, 1807, 3423, 3258, 4831, 5410, 259, 2586, 990, 904, 5372, 5123, 4891, 2414, 3470, 205, 151, 2294, 4512, 4925]\n",
      "Well, I do not mind telling you I have been at work upon this geometry of Four Dimensions for some time\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"Well, I do not mind telling you I have been at work upon this geometry\n",
    "          of Four Dimensions for some time\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4e6318",
   "metadata": {},
   "source": [
    "- Бүтін сандарды кері мәтінге декодтауға болады. \n",
    "- Мы можем декодировать целые числа обратно в текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da084ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Well, I do not mind telling you I have been at work upon this geometry of Four Dimensions for some time'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ad4d179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Well, I do not mind telling you I have been at work upon this geometry of Four Dimensions for some time'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ada69a",
   "metadata": {},
   "source": [
    "## 2.4 **Арнайы контекст** бар токендерін қосу \n",
    "## 2.4 Добавление **специальных контекстных** токенов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da8b0ad",
   "metadata": {},
   "source": [
    "- **Бейтаныс сөздерді** және **мәтіннің соңын** белгілеу үшін бірнеше «арнайы» токендер қосқан пайдалы.\n",
    "- Полезно добавить несколько «специальных» токенов для обозначения **неизвестных слов** и **конца текста**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96928ed7",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/09.webp?123\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996e9a3a",
   "metadata": {},
   "source": [
    "- Кейбір токенизаторлар АТМ-ге қосымша контекст беру үшін арнайы токендерді пайдаланады.  \n",
    "Осындай арнайы токендердің кейбірі:  \n",
    "    - [BOS] (тізбектің басы) мәтіннің басын белгілейді.\n",
    "    - [EOS] (тізбектің соңы) мәтіннің қай жерде аяқталатынын белгілейді (бұл әдетте бір-бірімен байланыссыз бірнеше мәтінді, мысалы, екі түрлі Уикипедия мақаласын немесе екі түрлі кітапты біріктіру үшін қолданылады).\n",
    "    - [PAD] (толтыру) егер АТМ-ді 1-ден үлкен топтама өлшемімен оқытсақ қолданылады (біз әртүрлі ұзындықтағы бірнеше мәтінді қосуымыз мүмкін; толтыру токенімен барлық мәтіндердің ұзындығы бірдей болуы үшін қысқа мәтіндерді ең ұзынына дейін толтырамыз).\n",
    "    - [UNK] сөздікке кірмеген сөздерді белгілеу үшін қолданылады.\n",
    "- Айта кететін жайт, `GPT-2` жоғарыда аталған токендердің ешқайсысын қажет етпейді, бірақ күрделілікті азайту үшін тек  `<|endoftext|>` токенін пайдаланады.\n",
    "`<|endoftext|>` жоғарыда аталған [EOS] токеніне ұқсас.  \n",
    "- GPT сондай-ақ `<|endoftext|>` токенін толтыру үшін де пайдаланады (себебі біз топтамалық енгізулермен оқыту кезінде әдетте масканы қолданамыз, сондықтан толтырылған токендерге назар аудармаймыз, сәйкесінше ол токендердің қандай болғаны маңызды емес).\n",
    "- GPT-2 сөздіктен тыс сөздер үшін <UNK> токенін пайдаланбайды; оның орнына GPT-2 байт жұбымен кодтау (BPE) токенизаторын қолданады, ол сөздерді біз кейінгі бөлімде талқылайтын сөз бөліктеріне бөледі."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3f26b1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- Некоторые токенизаторы используют специальные токены, чтобы предоставить LLM дополнительный контекст.   \n",
    "Вот некоторые из этих специальных токенов:\n",
    "    - [BOS] (начало последовательности) отмечает начало текста.\n",
    "    - [EOS] (конец последовательности) отмечает место окончания текста (обычно используется для объединения нескольких несвязанных текстов, например, двух разных статей из Википедии или двух разных книг и т. д.).\n",
    "    - [PAD] (заполнение/паддинг) используется, если мы обучаем LLM с размером батча больше 1 (мы можем включать несколько текстов разной длины; с помощью токена-заполнителя мы дополняем более короткие тексты до наибольшей длины, чтобы все тексты имели одинаковую длину).\n",
    "    - [UNK] используется для представления слов, которые не включены в словарь.\n",
    "\n",
    "- Обратите внимание, что `GPT-2` не требует ни одного из упомянутых выше токенов, а использует только токен `<|endoftext|>` для уменьшения сложности.  \n",
    "`<|endoftext|>` аналогичен упомянутому выше токену [EOS].\n",
    "- GPT также использует `<|endoftext|>` для паддинга (поскольку при обучении на пакетных данных мы обычно используем маску, мы всё равно не будем обращать внимание на токены-заполнители, поэтому не имеет значения, что это за токены).\n",
    "- GPT-2 не использует токен <UNK> для слов, отсутствующих в словаре; вместо этого GPT-2 использует токенизатор на основе попарного кодирования байтов (BPE), который разбивает слова на подслова (суб-единицы), что мы обсудим в следующем разделе."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0873ffd4",
   "metadata": {},
   "source": [
    "- Біз `<|endoftext|>` токендерін екі тәуелсіз мәтін көзінің арасында қолданамыз:\n",
    "- Мы используем токены `<|endoftext|>` между двумя независимыми источниками текста:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ca3796",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/10.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a884e662",
   "metadata": {},
   "source": [
    "- Төмендегі мәтінді `токендегенде` не болатынын көрейік:\n",
    "- Давайте посмотрим, что произойдёт, если мы `токенизируем` следующий текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f8bc05dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m tokenizer = SimpleTokenizerV1(vocab)\n\u001b[32m      3\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mHello, do you like tea. Is this-- a test?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mSimpleTokenizerV1.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     10\u001b[39m preprocessed = [\n\u001b[32m     11\u001b[39m     item.strip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item.strip()\n\u001b[32m     12\u001b[39m ]\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Convert strings to IDs\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m ids = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[31mKeyError\u001b[39m: 'Hello'"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"Hello, do you like tea. Is this-- a test?\"\n",
    "\n",
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4110a950",
   "metadata": {},
   "source": [
    "- Жоғарыдағы код қате шығарады, себебі `«Hello»` сөзі сөздікте жоқ.  \n",
    "- Мұндай жағдайларды шешу үшін сөздікке бейтаныс сөздерді белгілейтін `<|unk|>` сияқты арнайы токендерді қоса аламыз.\n",
    "- Сөздікті кеңейтіп жатқандықтан, GPT-2 оқытуында мәтіннің соңын белгілеу үшін қолданылатын `<|endoftext|>` деп аталатын тағы бір токен қосайық (ол сондай-ақ біріктірілген мәтіндер арасында да қолданылады, мысалы, егер оқыту деректер жиынымыз бірнеше мақаладан, кітаптан және т.б. тұрса)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00afa7dd",
   "metadata": {},
   "source": [
    "- Приведенный выше код выдаёт ошибку, потому что слово `«Hello»` отсутствует в словаре.\n",
    "- Чтобы справиться с такими случаями, мы можем добавить в словарь специальные токены, такие как `<|unk|>`, для представления неизвестных слов.\n",
    "- Поскольку мы уже расширяем словарь, давайте добавим ещё один токен, `<|endoftext|>`,  \n",
    "который используется при обучении GPT-2 для обозначения конца текста (он также используется между объединёнными текстами,    \n",
    "например, если наши обучающие датасеты состоят из нескольких статей, книг и т. д.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05f75731",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84964004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5444"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77fe4b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('”', 5439)\n",
      "('•', 5440)\n",
      "('…', 5441)\n",
      "('<|endoftext|>', 5442)\n",
      "('<|unk|>', 5443)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9921e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        # Preprocess the text\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()—*’“$%\\']|--|——|\\s)', text)\n",
    "        # Remove empty strings\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        # Replace unknown tokens\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        # Convert tokens to IDs\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        # Convert IDs back to tokens\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?_!\"()—*’“$%\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279ccb0b",
   "metadata": {},
   "source": [
    "- Өзгертілген токендеу құралымен мәтінді токендеп көрейік:\n",
    "- Давайте попробуем токенизировать текст с помощью изменённого токенизатора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9afa317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37e2e66f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5443,\n",
       " 6,\n",
       " 1807,\n",
       " 5410,\n",
       " 3078,\n",
       " 5443,\n",
       " 34,\n",
       " 5442,\n",
       " 274,\n",
       " 4859,\n",
       " 4749,\n",
       " 5443,\n",
       " 3470,\n",
       " 4859,\n",
       " 3573,\n",
       " 8]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "197d1a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like <|unk|>? <|endoftext|> In the sunlit <|unk|> of the palace.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a665eb2c",
   "metadata": {},
   "source": [
    "## 2.5 BytePair кодтау (Байт жұбымен кодтау)\n",
    "## 2.5 BytePair кодирование (Попарное кодирование байтов)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6249b4a4",
   "metadata": {},
   "source": [
    "- GPT-2 өз токендегіші ретінде BytePair (Байт жұбымен кодтауды) (BPE) пайдаланды.\n",
    "- Бұл модельге алдын ала анықталған сөздікте жоқ сөздерді кішірек сөз бөліктеріне немесе тіпті жеке таңбаларға бөлуге мүмкіндік береді, осылайша ол сөздіктен тыс сөздерді өңдей алады.\n",
    "- Мысалы, егер GPT-2 сөздігінде `«unfamiliarword»` деген сөз болмаса, ол оны өзінің үйретілген BPE біріктірулеріне байланысты `[\"unfam\", \"iliar\", \"word\"]` немесе басқа сөз бөліктеріне бөліп токендеуі мүмкін.   \n",
    "**Мысалы:**    \n",
    "`Walker walked a long way` -> `[w, a, l, k, e, r, d, o, n, g, y, al, alk, alke, walke]`\n",
    "***\n",
    "- Түпнұсқа BPE токендегішін мына жерден табуға болады: [https://github.com/openai/gpt-2/blob/master/src/encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py)\n",
    "- Бұл тарауда біз OpenAI-дың ашық бастапқы кодты `tiktoken` пакеттың `BPE` токендегішін қолданамыз. Бұл пакеттың негізгі алгоритмдері есептеу өнімділігін жақсарту үшін Rust тілінде жазылған."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3b8995",
   "metadata": {},
   "source": [
    "- GPT-2 использовал попарное кодирование байтов (BPE) в качестве своего токенизатора.\n",
    "- Это позволяет модели разбивать слова, которых нет в её предопределённом словаре, на более мелкие подслова или даже отдельные символы, что даёт возможность обрабатывать слова, отсутствующие в словаре.\n",
    "- Например, если в словаре GPT-2 нет слова «unfamiliarword», он может токенизировать его как [\"unfam\", \"iliar\", \"word\"] или разбить на другие подслова, в зависимости от своих обученных слияний BPE.    \n",
    "**Пример:**    \n",
    "`Walker walked a long way` -> `[w, a, l, k, e, r, d, o, n, g, y, al, alk, alke, walke]`  \n",
    "***\n",
    "- Оригинальный токенизатор BPE можно найти здесь: [https://github.com/openai/gpt-2/blob/master/src/encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py)\n",
    "- В этой главе мы используем токенизатор BPE из библиотеки с открытым исходным кодом tiktoken от OpenAI, которая реализует свои основные алгоритмы на Rust для повышения вычислительной производительности."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bda6a67",
   "metadata": {},
   "source": [
    "## LINK\n",
    "* BPE visualizer: https://www.bpe-visualizer.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7cea2053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.12.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c33ca6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f077e715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36ff698c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defbf4df",
   "metadata": {},
   "source": [
    "- BPE токендегіштері бейтаныс сөздерді сөз бөліктеріне және жеке таңбаларға бөледі:\n",
    "- BPE-токенизаторы разбивают неизвестные слова на подслова и отдельные символы:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b595c654",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/11.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559fd044",
   "metadata": {},
   "source": [
    "## 2.6 Жылжымалы терезе әдісі арқылы деректерді іріктеу\n",
    "## 2.6 Выборка данных методом скользящего окна"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3150eb13",
   "metadata": {},
   "source": [
    "- Біз АТМ-дерді  бір сөз генерациялауға үйрететіндіктен, оқыту деректерде келесі сөз  ағымдағы сөзге болжау үшін нысана болатындай етіп дайындаймыз:\n",
    "- Мы обучаем LLM генерировать по одному слову за раз, поэтому обучающие данные нужно подготовить так, чтобы следующее слово в последовательности было целью для предсказания :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd0d05e",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/12.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d34d25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49509\n"
     ]
    }
   ],
   "source": [
    "with open(\"time_machine.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "# Encoding text using new tokenizer\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45327306",
   "metadata": {},
   "source": [
    "- Әрбір мәтін бөлігі үшін бізге кіріс деректер мен нысана деректер қажет.\n",
    "- Модельдің келесі сөзді болжағанын қалайтындықтан, нысаналар дегеніміз — оңға қарай бір позицияға ығыстырылған кіріс деректер.\n",
    "\n",
    "<br>\n",
    "\n",
    "- Для каждого фрагмента текста нам нужны входящие данные и целевые данные.  \n",
    "- Поскольку мы хотим, чтобы модель предсказывала следующее слово, цели — это входы, сдвинутые на одну позицию вправо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd7aa336",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd1b6120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [373, 44869, 290, 15108]\n",
      "y:      [44869, 290, 15108, 13]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff270892",
   "metadata": {},
   "source": [
    "- Болжау әдісі төмендегідей болады:\n",
    "- Шаг за шагом предсказание будет выглядеть следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b38dd880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[373] ----> 44869\n",
      "[373, 44869] ----> 290\n",
      "[373, 44869, 290] ----> 15108\n",
      "[373, 44869, 290, 15108] ----> 13\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "85ee4331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " was ---->  flushed\n",
      " was flushed ---->  and\n",
      " was flushed and ---->  animated\n",
      " was flushed and animated ----> .\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9ab12f",
   "metadata": {},
   "source": [
    "- Келесі сөзді болжауды біз `зейін механизмін` (attention mechanism) қарастырып болғаннан кейін, кейінгі тарауда талқылаймыз.\n",
    "- Әзірше біз кіріс деректер жиынын топтастап, бір позицияға ығыстырылған кіріс деректер мен нысана деректерді қайтаратын қарапайым деректер жүктегішін жүзеге асырамыз.\n",
    "\n",
    "- Мы займёмся предсказанием следующего слова в одной из последующих глав, после того как изучим механизм внимания (attention mechanism).\n",
    "- А пока мы реализуем простой загрузчик данных (dataloader), который проходит по входному набору данных и возвращает входы и цели, сдвинутые на одну позицию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a77493be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f1b0e7",
   "metadata": {},
   "source": [
    "- Позицияны +1 сөзге өзгерте отырып, жылжымалы терезе әдісін қолданамыз:\n",
    "- Мы используем подход скользящего окна, смещая позицию на +1 слово:\n",
    "\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/13.webp?123\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384a92aa",
   "metadata": {},
   "source": [
    "- Кіріс мәтіндік деректер жиынынан бөліктерді алатын деректер жиынын (dataset) және деректер жүктегішін (dataloader) құру.\n",
    "- Создание набора данных (dataset) и загрузчика данных (dataloader), которые извлекают фрагменты из входного текстового набора данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f20441fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        assert len(token_ids) > max_length, \"Number of tokenized inputs must at least be equal to max_length+1\"\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            # The input is a chunk of text of size 'max_length'\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            # The target is the same chunk, but shifted one position to the right.\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "\n",
    "            # Convert the chunks to PyTorch tensors and store them.\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    # Returns the total number of samples in the dataset.\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "        \n",
    "    # Retrieves one sample (an input and its corresponding target) from the dataset.\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc95d2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941bd30e",
   "metadata": {},
   "source": [
    "- Контекст өлшемі (сөздер саны) 4 болатын АТМ үшін топтама өлшемі 1-ге тең деректер жүктегішін (dataloader) тексеріп көрейік:\n",
    "- Давайте протестируем загрузчик данных (dataloder) с размером батча 1 для LLM с размером контекста 4:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "25db9599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  314,    13,   198, 22395]]), tensor([[   13,   198, 22395,   628]])]\n"
     ]
    }
   ],
   "source": [
    "with open(\"time_machine.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# Create a dataloader instance using our custom function.\n",
    "dataloader = create_dataloader_v1(\n",
    "                raw_text,           # The source text to be processed into chunks.\n",
    "                batch_size=1,       # The number of text chunks in each batch.\n",
    "                max_length=4,       # The fixed length of each text chunk (in tokens).\n",
    "                stride=1,           # The step size to slide the window for creating chunks.\n",
    "                shuffle=False       # Whether to shuffle the order of the chunks (False means keep order).\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "decb89ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[   13,   198, 22395,   628]]), tensor([[  198, 22395,   628,   198]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc545125",
   "metadata": {},
   "source": [
    "- Төменде қадамы контекст ұзындығына (бұл жерде: 4) тең болатын мысал көрсетілген:\n",
    "- Ниже приведён пример, в котором шаг равен длине контекста (здесь: 4):\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/14.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef381d71",
   "metadata": {},
   "source": [
    "- Біз сондай-ақ  нәтижелер деректерін  топтамамен (batch) де шығара аламыз.\n",
    "- Ескеріңіз, біз бұл жерде топтамалар арасында қиылысулар болмауы үшін қадамды ұлғайтамыз, себебі қиылысудың көп болуы шамадан тыс үйретудің артуына әкелуі мүмкін.\n",
    "\n",
    "<br>\n",
    "\n",
    "- Мы также можем создавать пакетные (batch) выходные данные.\n",
    "- Обратите внимание, что здесь мы увеличиваем шаг, чтобы не было пересечений между пакетами, поскольку большее пересечение может привести к усилению переобучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b17be362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[  314,    13,   198, 22395],\n",
      "        [  628,   198,   464,  3862],\n",
      "        [43662,  6051,   357,  1640],\n",
      "        [  523,   340,   481,   307],\n",
      "        [11282,   284,  2740,   286],\n",
      "        [  683,     8,   373,   198],\n",
      "        [11201,  9969,   257,   664],\n",
      "        [  623,   578,  2300,   284]])\n",
      "\n",
      "Targets:\n",
      " tensor([[   13,   198, 22395,   628],\n",
      "        [  198,   464,  3862, 43662],\n",
      "        [ 6051,   357,  1640,   523],\n",
      "        [  340,   481,   307, 11282],\n",
      "        [  284,  2740,   286,   683],\n",
      "        [    8,   373,   198, 11201],\n",
      "        [ 9969,   257,   664,   623],\n",
      "        [  578,  2300,   284,   514]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "                raw_text, \n",
    "                batch_size=8, \n",
    "                max_length=4, \n",
    "                stride=4, \n",
    "                shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4dc230",
   "metadata": {},
   "source": [
    "## 2.7 Токен эмбеддингтерін құру\n",
    "## 2.7 Создание эмбеддинги токенов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1446ad",
   "metadata": {},
   "source": [
    "- Деректер LLM (АТМ) үйрету үшін дайын деуге болады.\n",
    "- Соңында, эмбеддинг қабатын пайдаланып, токендерді үздіксіз векторлық көрініске (vectors) эмбеддейік.\n",
    "- Әдетте, бұл эмбеддинг қабаттары LLM-нің өз құрамына кіреді және модельді оқыту барысында жаңартылып (оқытылып) отырады.\n",
    "\n",
    "<br>\n",
    "\n",
    "- Данные уже почти готовы для обучения LLM.\n",
    "- И наконец, давайте преобразуем токены в непрерывное векторное представление с помощью слоя эмбеддингов.\n",
    "- Обычно эти слои эмбеддингов являются частью самой LLM и обновляются (обучаются) в процессе обучения модели.\n",
    "\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/15.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d05d07",
   "metadata": {},
   "source": [
    "- Бізде (токендеуден кейін) кіріс ID-лары 2, 3, 5 және 1 болатын келесі төрт кіріс мысалы бар деп есептейік.\n",
    "- Предположим, у нас есть следующие четыре входных примера с ID 2, 3, 5 и 1 (после токенизации)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "70feeb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b41be72",
   "metadata": {},
   "source": [
    "- Қарапайымдылық үшін, бізде бар болғаны 6 сөзден тұратын шағын сөздік бар және біз өлшемі 3-ке тең эмбеддингтер жасағымыз келеді деп есептейік:\n",
    "- Для простоты предположим, что у нас есть небольшой словарь всего из 6 слов, и мы хотим создать эмбеддинги размером 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2f5d64b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "95e6e303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247b91e2",
   "metadata": {},
   "source": [
    "- ID-і 3-ке тең токенді 3-өлшемді векторға түрлендіру үшін келесіні орындаймыз:\n",
    "- Чтобы преобразовать токен с ID 3 в 3-мерный вектор, выполним следующее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "305ac89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3374, -0.1778, -0.1690]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c091f408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 5, 1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "26a09883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379d78b7",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/16.webp?123\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d2cf38",
   "metadata": {},
   "source": [
    "## 2.8 Сөз позицияларын кодтау\n",
    "## 2.8 Кодирование позиций слов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a945b4",
   "metadata": {},
   "source": [
    "- Эмбеддинг қабаты токендерды деректерде \n",
    "орналасу орнын есепке алмай векторлық көріністерге айналдыра береді:\n",
    "- Слой эмбеддингов преобразует токены в идентичные векторные представления независимо от их расположения во входной последовательности:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80091c95",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/17.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368d50a6",
   "metadata": {},
   "source": [
    "- **Позициялық эмбеддингтер** **ауқымды тілдік модель** үшін **кіріс эмбеддингтерін** құру мақсатында **токен эмбеддинг векторымен** біріктіріледі:\n",
    "\n",
    "-   **Позиционные эмбеддинги** объединяются с **вектором эмбеддинга токена**, чтобы сформировать **входные эмбеддинги** для **большой языковой модели**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f03195",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/18.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc185fca",
   "metadata": {},
   "source": [
    "-   **Байт жұбымен кодтаушының (BytePair encoder)** **сөздік қорының өлшемі 50 257-ге тең**:\n",
    "-   **Кіріс токендерін** **256-өлшемді векторлық көрініске** кодтағымыз келеді деп есептейік:\n",
    "\n",
    "<br>\n",
    "\n",
    "-   **Кодировщик BytePair** имеет **размер словаря 50 257**:\n",
    "-   Предположим, мы хотим закодировать **входные токены** в **256-мерное векторное представление**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "266f0192",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac809748",
   "metadata": {},
   "source": [
    "- Деректер жүктегішінен (dataloader) деректерді іріктесек, біз әрбір топтамадағы токендерді 256-өлшемді векторға эмбеддейміз.\n",
    "- Егер бізде әрқайсысында 4 токені бар топтама (batch) өлшемі 8 болса, нәтижесінде 8 x 4 x 256 тензоры пайда болады:\n",
    "\n",
    "<br>\n",
    "\n",
    "- Если мы делаем выборку данных из загрузчика (dataloader), мы преобразуем токены в каждом пакете в 256-мерный вектор.\n",
    "- Если у нас размер пакета (батча) равен 8 и в каждом по 4 токена, в результате мы получим тензор размером 8 x 4 x 256:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a5ab44bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "                raw_text, \n",
    "                batch_size=8, \n",
    "                max_length=max_length,\n",
    "                stride=max_length, \n",
    "                shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "be579628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[  314,    13,   198, 22395],\n",
      "        [  628,   198,   464,  3862],\n",
      "        [43662,  6051,   357,  1640],\n",
      "        [  523,   340,   481,   307],\n",
      "        [11282,   284,  2740,   286],\n",
      "        [  683,     8,   373,   198],\n",
      "        [11201,  9969,   257,   664],\n",
      "        [  623,   578,  2300,   284]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8a8a0bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)\n",
    "\n",
    "# First group of words\n",
    "# print(token_embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e420dfe",
   "metadata": {},
   "source": [
    "- GPT-2 абсолютті позициялық эмбеддингтерді пайдаланады, \n",
    "сондықтан біз жай ғана тағы бір эмбеддинг қабатын құрамыз:\n",
    "- GPT-2 использует абсолютные позиционные эмбеддинги, \n",
    "поэтому мы просто создаём ещё один слой эмбеддингов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a3b65aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "\n",
    "# print(pos_embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "da3922b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n",
      "tensor([[ 1.7375, -0.5620, -0.6303,  ..., -0.2277,  1.5748,  1.0345],\n",
      "        [ 1.6423, -0.7201,  0.2062,  ...,  0.4118,  0.1498, -0.4628],\n",
      "        [-0.4651, -0.7757,  0.5806,  ...,  1.4335, -0.4963,  0.8579],\n",
      "        [-0.6754, -0.4628,  1.4323,  ...,  0.8139, -0.7088,  0.4827]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)\n",
    "\n",
    "print(pos_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "59e4e23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5786, -1.8926, -1.7647,  ..., -0.1368,  0.3491,  2.1122],\n",
      "        [-1.1577, -1.9382,  0.9027,  ..., -0.0718, -0.8468, -1.0623],\n",
      "        [-0.1437, -0.9782,  1.5918,  ...,  0.3068, -1.1135, -0.7202],\n",
      "        [ 0.3407, -0.9464,  1.4344,  ..., -1.5509, -1.9549,  1.4846]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(token_embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e51698b",
   "metadata": {},
   "source": [
    "- LLM-де қолданылатын кіріс эмбеддингтерін құру үшін біз токендік және позициялық эмбеддингтерді жай ғана қосамыз:\n",
    "- Чтобы создать входные эмбеддинги, используемые в LLM, мы просто складываем токенные и позиционные эмбеддинги:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "45306ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n",
      "tensor([[ 2.3161, -2.4545, -2.3950,  ..., -0.3645,  1.9240,  3.1467],\n",
      "        [ 0.4845, -2.6583,  1.1089,  ...,  0.3400, -0.6970, -1.5251],\n",
      "        [-0.6088, -1.7539,  2.1724,  ...,  1.7403, -1.6098,  0.1377],\n",
      "        [-0.3347, -1.4092,  2.8668,  ..., -0.7369, -2.6637,  1.9673]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)\n",
    "\n",
    "print(input_embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79dd3af",
   "metadata": {},
   "source": [
    "- Кіріс деректерді өңдеу процесінің бастапқы кезеңінде кіріс мәтіні жеке токендерге бөлінеді.\n",
    "- Осы сегменттеуден кейін бұл токендер алдын ала анықталған сөздік негізінде токен ID-ларына айналдырылады:\n",
    "\n",
    "<br>\n",
    "\n",
    "- На начальном этапе процесса обработки входных данных входной текст сегментируется на отдельные токены.\n",
    "- После этой сегментации эти токены преобразуются в идентификаторы токенов (ID) на основе предопределённого словаря:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483c3683",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/19.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93180d55",
   "metadata": {},
   "source": [
    "# Links\n",
    "\n",
    "* BPE visualizer: https://www.bpe-visualizer.com\n",
    "* Tokenizer visualizer: https://tiktokenizer.vercel.app"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gulnur_gv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
